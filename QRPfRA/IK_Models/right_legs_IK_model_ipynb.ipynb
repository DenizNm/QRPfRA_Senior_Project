{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gymnasium import utils\n",
    "from gymnasium.envs.mujoco import MujocoEnv\n",
    "from gymnasium.error import Error\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.utils.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRPfRA_v3(MujocoEnv, utils.EzPickle):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\n",
    "            \"human\",\n",
    "            \"rgb_array\",\n",
    "            \"depth_array\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(self, xml_file=\"/Users/deniz/PycharmProjects/QRPfRA_Senior_Project/QRPfRA/IK_Models/qrpfra_v3_leg_ik_scene_right.xml\", frame_skip=1, **kwargs):\n",
    "        utils.EzPickle.__init__(self, xml_file, frame_skip, **kwargs)\n",
    "\n",
    "        MujocoEnv.__init__(\n",
    "            self,\n",
    "            xml_file,\n",
    "            frame_skip=frame_skip,\n",
    "            observation_space=None,  # needs to be defined after\n",
    "            default_camera_config={},\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.metadata = {\n",
    "            \"render_modes\": [\n",
    "                \"human\",\n",
    "                \"rgb_array\",\n",
    "                \"depth_array\",\n",
    "            ],\n",
    "            \"render_fps\": int(np.round(1.0 / self.dt)),\n",
    "        }\n",
    "\n",
    "        obs_size = self.data.qpos.size + self.data.qvel.size\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        x_position_before = self.data.qpos[0]\n",
    "\n",
    "        self.do_simulation(action, self.frame_skip)\n",
    "        x_position_after = self.data.qpos[0]\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        reward = x_position_after - x_position_before\n",
    "        info = {}\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return observation, reward, False, False, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        #position = self.data.qpos.flat.copy()\n",
    "        #velocity = self.data.qvel.flat.copy()\n",
    "        sensor_data = self.data.sensordata.flat.copy()\n",
    "\n",
    "        return sensor_data\n",
    "\n",
    "    def reset_model(self):\n",
    "        qpos = self.init_qpos\n",
    "        qvel = self.init_qvel\n",
    "        self.set_state(qpos, qvel)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def _get_reset_info(self):\n",
    "        return {\"works\": True}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.045 ,  0.1335, -0.107 ,  0.    ,  0.    ,  0.    ]), {'works': True})\n"
     ]
    }
   ],
   "source": [
    "env = QRPfRA_v3()\n",
    "check_env(env)\n",
    "obs = env.reset()\n",
    "print(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_leg_dataset = [] # [obs0, obs1, obs2, act0, act1, act2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-100, 100):\n",
    "    for j in range(-100, 100):\n",
    "        for k in range(-100, 100):\n",
    "            #env.render_mode = \"human\"\n",
    "            env_obs = []\n",
    "            for step_cnt in range(0,12):\n",
    "                obs, reward, done, _, info = env.step([float(m) for m in [i, j, k]])\n",
    "                if step_cnt == 11:\n",
    "                    env_obs = obs\n",
    "\n",
    "            left_leg_dataset.append([env_obs[0], env_obs[1], env_obs[2], i/100, j/100, k/100])\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000\n"
     ]
    }
   ],
   "source": [
    "print(len(left_leg_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert left leg list to numpy array\n",
    "left_leg_dataset_as_np = np.array(left_leg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17738146 -0.14628534 -0.03948343 -1.         -1.         -1.        ]\n",
      " [ 0.17787466 -0.1452782  -0.04005906 -1.         -1.         -0.99      ]\n",
      " [ 0.17841268 -0.1441787  -0.04036878 -1.         -1.         -0.98      ]\n",
      " [ 0.1789408  -0.14307237 -0.0406731  -1.         -1.         -0.97      ]]\n",
      "[[ 0.17738146 -0.14628534 -0.03948343]\n",
      " [ 0.17787466 -0.1452782  -0.04005906]\n",
      " [ 0.17841268 -0.1441787  -0.04036878]\n",
      " ...\n",
      " [ 0.06802939  0.04272428 -0.00187604]\n",
      " [ 0.06750877  0.04161429 -0.00218015]\n",
      " [ 0.06697809  0.04051071 -0.00249014]]\n",
      "[[-1.   -1.   -1.  ]\n",
      " [-1.   -1.   -0.99]\n",
      " [-1.   -1.   -0.98]\n",
      " ...\n",
      " [ 0.99  0.99  0.97]\n",
      " [ 0.99  0.99  0.98]\n",
      " [ 0.99  0.99  0.99]]\n"
     ]
    }
   ],
   "source": [
    "print(left_leg_dataset_as_np[0:4])\n",
    "\n",
    "observations = left_leg_dataset_as_np[:, 0:3][:]\n",
    "print(observations)\n",
    "actions = left_leg_dataset_as_np[:, 3:][:]\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7032/7032 [==============================] - 12s 2ms/step - loss: 0.0247 - accuracy: 0.9267\n",
      "Epoch 2/10\n",
      "7032/7032 [==============================] - 11s 2ms/step - loss: 0.0152 - accuracy: 0.9494\n",
      "Epoch 3/10\n",
      "7032/7032 [==============================] - 12s 2ms/step - loss: 0.0147 - accuracy: 0.9510\n",
      "Epoch 4/10\n",
      "7032/7032 [==============================] - 12s 2ms/step - loss: 0.0142 - accuracy: 0.9519\n",
      "Epoch 5/10\n",
      "7032/7032 [==============================] - 11s 2ms/step - loss: 0.0139 - accuracy: 0.9523\n",
      "Epoch 6/10\n",
      "7032/7032 [==============================] - 11s 2ms/step - loss: 0.0136 - accuracy: 0.9532\n",
      "Epoch 7/10\n",
      "7032/7032 [==============================] - 12s 2ms/step - loss: 0.0134 - accuracy: 0.9548\n",
      "Epoch 8/10\n",
      "7032/7032 [==============================] - 11s 2ms/step - loss: 0.0133 - accuracy: 0.9558\n",
      "Epoch 9/10\n",
      "7032/7032 [==============================] - 11s 2ms/step - loss: 0.0132 - accuracy: 0.9570\n",
      "Epoch 10/10\n",
      "7032/7032 [==============================] - 11s 2ms/step - loss: 0.0131 - accuracy: 0.9575\n",
      "25000/25000 [==============================] - 10s 391us/step - loss: 0.0133 - accuracy: 0.9559\n",
      "Test Loss: [0.013338177464902401, 0.9559475183486938]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_obs, test_obs, train_actions, test_actions = train_test_split(observations, actions, test_size=0.1, shuffle=True)\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, activation='relu', input_shape=(3,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='tanh')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_obs, train_actions, epochs=10, batch_size=1024)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_loss = model.evaluate(test_obs, test_actions)\n",
    "print('Test Loss:', test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: right_legs_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: right_legs_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"right_legs_model\", overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QRPfRA_Senior_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
