{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gymnasium import utils\n",
    "from gymnasium.envs.mujoco import MujocoEnv\n",
    "from gymnasium.error import Error\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.utils.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRPfRA_v3(MujocoEnv, utils.EzPickle):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\n",
    "            \"human\",\n",
    "            \"rgb_array\",\n",
    "            \"depth_array\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(self, xml_file=\"/Users/deniz/PycharmProjects/QRPfRA_Senior_Project/QRPfRA/IK_Models/qrpfra_v3_leg_ik_scene_left.xml\", frame_skip=1, **kwargs):\n",
    "        utils.EzPickle.__init__(self, xml_file, frame_skip, **kwargs)\n",
    "\n",
    "        MujocoEnv.__init__(\n",
    "            self,\n",
    "            xml_file,\n",
    "            frame_skip=frame_skip,\n",
    "            observation_space=None,  # needs to be defined after\n",
    "            default_camera_config={},\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.metadata = {\n",
    "            \"render_modes\": [\n",
    "                \"human\",\n",
    "                \"rgb_array\",\n",
    "                \"depth_array\",\n",
    "            ],\n",
    "            \"render_fps\": int(np.round(1.0 / self.dt)),\n",
    "        }\n",
    "\n",
    "        obs_size = self.data.qpos.size + self.data.qvel.size\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        x_position_before = self.data.qpos[0]\n",
    "\n",
    "        self.do_simulation(action, self.frame_skip)\n",
    "        x_position_after = self.data.qpos[0]\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        reward = x_position_after - x_position_before\n",
    "        info = {}\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return observation, reward, False, False, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        #position = self.data.qpos.flat.copy()\n",
    "        #velocity = self.data.qvel.flat.copy()\n",
    "        sensor_data = self.data.sensordata.flat.copy()\n",
    "\n",
    "        return sensor_data\n",
    "\n",
    "    def reset_model(self):\n",
    "        qpos = self.init_qpos\n",
    "        qvel = self.init_qvel\n",
    "        self.set_state(qpos, qvel)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def _get_reset_info(self):\n",
    "        return {\"works\": True}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.045 ,  0.1335, -0.107 ,  0.    ,  0.    ,  0.    ]), {'works': True})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deniz/miniconda3/envs/QRPfRA_Senior_Project/lib/python3.11/site-packages/gymnasium/utils/env_checker.py:247: UserWarning: \u001b[33mWARN: For Box action spaces, we recommend using a symmetric and normalized space (range=[-1, 1] or [0, 1]). See https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html for more information.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/deniz/miniconda3/envs/QRPfRA_Senior_Project/lib/python3.11/site-packages/gymnasium/utils/env_checker.py:225: UserWarning: \u001b[33mWARN: A Box observation space minimum value is -infinity. This is probably too low.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/deniz/miniconda3/envs/QRPfRA_Senior_Project/lib/python3.11/site-packages/gymnasium/utils/env_checker.py:229: UserWarning: \u001b[33mWARN: A Box observation space maximum value is -infinity. This is probably too high.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/deniz/miniconda3/envs/QRPfRA_Senior_Project/lib/python3.11/site-packages/gymnasium/utils/env_checker.py:321: UserWarning: \u001b[33mWARN: Not able to test alternative render modes due to the environment not having a spec. Try instantialising the environment through gymnasium.make\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = QRPfRA_v3()\n",
    "check_env(env)\n",
    "obs = env.reset()\n",
    "print(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_leg_dataset = [] # [obs0, obs1, obs2, act0, act1, act2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-100, 100):\n",
    "    for j in range(-100, 100):\n",
    "        for k in range(-100, 100):\n",
    "            #env.render_mode = \"human\"\n",
    "            env_obs = []\n",
    "            for step_cnt in range(0,8):\n",
    "                obs, reward, done, _, info = env.step([float(m) for m in [i, j, k]])\n",
    "                if step_cnt == 7:\n",
    "                    env_obs = obs\n",
    "\n",
    "            left_leg_dataset.append([env_obs[0], env_obs[1], env_obs[2], i/100, j/100, k/100])\n",
    "\n",
    "            env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000\n"
     ]
    }
   ],
   "source": [
    "print(len(left_leg_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert left leg list to numpy array\n",
    "left_leg_dataset_as_np = np.array(left_leg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1318721  -0.14951101 -0.11471615 -1.         -1.         -1.        ]\n",
      " [ 0.13245049 -0.14842287 -0.11503899 -1.         -1.         -0.99      ]\n",
      " [ 0.13301877 -0.14732776 -0.11535618 -1.         -1.         -0.98      ]\n",
      " [ 0.13357687 -0.14622581 -0.11566769 -1.         -1.         -0.97      ]]\n",
      "[[ 0.1318721  -0.14951101 -0.11471615]\n",
      " [ 0.13245049 -0.14842287 -0.11503899]\n",
      " [ 0.13301877 -0.14732776 -0.11535618]\n",
      " ...\n",
      " [ 0.02308279  0.04131714  0.07558345]\n",
      " [ 0.02253902  0.04020884  0.07527202]\n",
      " [ 0.02198508  0.03910725  0.07495476]]\n",
      "[[-1.   -1.   -1.  ]\n",
      " [-1.   -1.   -0.99]\n",
      " [-1.   -1.   -0.98]\n",
      " ...\n",
      " [ 0.99  0.99  0.97]\n",
      " [ 0.99  0.99  0.98]\n",
      " [ 0.99  0.99  0.99]]\n"
     ]
    }
   ],
   "source": [
    "print(left_leg_dataset_as_np[0:4])\n",
    "\n",
    "observations = left_leg_dataset_as_np[:, 0:3][:]\n",
    "print(observations)\n",
    "actions = left_leg_dataset_as_np[:, 3:][:]\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7032/7032 [==============================] - 8s 1ms/step - loss: 0.0234 - accuracy: 0.9097\n",
      "Epoch 2/10\n",
      "7032/7032 [==============================] - 8s 1ms/step - loss: 0.0154 - accuracy: 0.9307\n",
      "Epoch 3/10\n",
      "7032/7032 [==============================] - 8s 1ms/step - loss: 0.0149 - accuracy: 0.9341\n",
      "Epoch 4/10\n",
      "7032/7032 [==============================] - 8s 1ms/step - loss: 0.0145 - accuracy: 0.9364\n",
      "Epoch 5/10\n",
      "7032/7032 [==============================] - 8s 1ms/step - loss: 0.0143 - accuracy: 0.9381\n",
      "Epoch 6/10\n",
      "7032/7032 [==============================] - 8s 1ms/step - loss: 0.0140 - accuracy: 0.9400\n",
      "Epoch 7/10\n",
      "7032/7032 [==============================] - 8s 1ms/step - loss: 0.0139 - accuracy: 0.9407\n",
      "Epoch 8/10\n",
      "7032/7032 [==============================] - 8s 1ms/step - loss: 0.0138 - accuracy: 0.9414\n",
      "Epoch 9/10\n",
      "7032/7032 [==============================] - 8s 1ms/step - loss: 0.0137 - accuracy: 0.9417\n",
      "Epoch 10/10\n",
      "7032/7032 [==============================] - 8s 1ms/step - loss: 0.0137 - accuracy: 0.9419\n",
      "25000/25000 [==============================] - 9s 342us/step - loss: 0.0135 - accuracy: 0.9399\n",
      "Test Loss: [0.01349296048283577, 0.9398987293243408]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_obs, test_obs, train_actions, test_actions = train_test_split(observations, actions, test_size=0.1)\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, activation='relu', input_shape=(3,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='tanh')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_obs, train_actions, epochs=10, batch_size=1024)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_loss = model.evaluate(test_obs, test_actions)\n",
    "print('Test Loss:', test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: left_legs_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: left_legs_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"left_legs_model\", overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QRPfRA_Senior_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
