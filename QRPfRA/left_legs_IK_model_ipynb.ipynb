{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gymnasium import utils\n",
    "from gymnasium.envs.mujoco import MujocoEnv\n",
    "from gymnasium.error import Error\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium.utils.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRPfRA_v3(MujocoEnv, utils.EzPickle):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\n",
    "            \"human\",\n",
    "            \"rgb_array\",\n",
    "            \"depth_array\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(self, xml_file=\"/Users/deniz/PycharmProjects/QRPfRA_Senior_Project/QRPfRA/IK_Models/qrpfra_v3_leg_ik_scene_left.xml\", frame_skip=1, **kwargs):\n",
    "        utils.EzPickle.__init__(self, xml_file, frame_skip, **kwargs)\n",
    "\n",
    "        MujocoEnv.__init__(\n",
    "            self,\n",
    "            xml_file,\n",
    "            frame_skip=frame_skip,\n",
    "            observation_space=None,  # needs to be defined after\n",
    "            default_camera_config={},\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.metadata = {\n",
    "            \"render_modes\": [\n",
    "                \"human\",\n",
    "                \"rgb_array\",\n",
    "                \"depth_array\",\n",
    "            ],\n",
    "            \"render_fps\": int(np.round(1.0 / self.dt)),\n",
    "        }\n",
    "\n",
    "        obs_size = self.data.qpos.size + self.data.qvel.size\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float64\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        self.do_simulation(action, self.frame_skip)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return observation, reward, False, False, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        #position = self.data.qpos.flat.copy()\n",
    "        #velocity = self.data.qvel.flat.copy()\n",
    "        sensor_data = self.data.sensordata.flat.copy()\n",
    "\n",
    "        return sensor_data\n",
    "\n",
    "    def reset_model(self):\n",
    "        qpos = self.init_qpos\n",
    "        qvel = self.init_qvel\n",
    "        self.set_state(qpos, qvel)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def _get_reset_info(self):\n",
    "        return {\"works\": True}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.046 ,  0.1435, -0.1245,  0.    ,  0.    ,  0.    ]), {'works': True})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deniz/miniconda3/envs/QRPfRA_Senior_Project/lib/python3.11/site-packages/gymnasium/utils/env_checker.py:247: UserWarning: \u001b[33mWARN: For Box action spaces, we recommend using a symmetric and normalized space (range=[-1, 1] or [0, 1]). See https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html for more information.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/deniz/miniconda3/envs/QRPfRA_Senior_Project/lib/python3.11/site-packages/gymnasium/utils/env_checker.py:225: UserWarning: \u001b[33mWARN: A Box observation space minimum value is -infinity. This is probably too low.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/deniz/miniconda3/envs/QRPfRA_Senior_Project/lib/python3.11/site-packages/gymnasium/utils/env_checker.py:229: UserWarning: \u001b[33mWARN: A Box observation space maximum value is -infinity. This is probably too high.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/deniz/miniconda3/envs/QRPfRA_Senior_Project/lib/python3.11/site-packages/gymnasium/utils/env_checker.py:321: UserWarning: \u001b[33mWARN: Not able to test alternative render modes due to the environment not having a spec. Try instantialising the environment through gymnasium.make\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = QRPfRA_v3()\n",
    "check_env(env)\n",
    "obs = env.reset()\n",
    "print(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_dataset = [] # [obs0, obs1, obs2, act0, act1, act2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-100, 100):\n",
    "    for j in range(-100, 100):\n",
    "        for k in range(-100, 100):\n",
    "            #env.render_mode = \"human\"\n",
    "            env_obs = []\n",
    "            for step_cnt in range(0,6):\n",
    "                obs, reward, done, _, info = env.step([float(m) for m in [i, j, k]])\n",
    "                if step_cnt == 5:\n",
    "                    env_obs = obs\n",
    "            \n",
    "\n",
    "            leg_dataset.append([env_obs[0], env_obs[1], env_obs[2], i/100, j/100, k/100])\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8141912\n"
     ]
    }
   ],
   "source": [
    "print(len(leg_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert left leg list to numpy array\n",
    "leg_dataset_as_np = np.array(leg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13712376 -0.15670043 -0.13300666 -1.         -1.         -1.        ]\n",
      " [ 0.14307411 -0.14464797 -0.13652958 -1.         -1.         -0.9       ]\n",
      " [ 0.14786721 -0.1321218  -0.13926268 -1.         -1.         -0.8       ]\n",
      " [ 0.15149051 -0.11908522 -0.14132876 -1.         -1.         -0.7       ]]\n",
      "[[ 0.13712376 -0.15670043 -0.13300666]\n",
      " [ 0.14307411 -0.14464797 -0.13652958]\n",
      " [ 0.14786721 -0.1321218  -0.13926268]\n",
      " ...\n",
      " [ 0.03177611  0.04240239  0.06983805]\n",
      " [ 0.03125204  0.04117479  0.06953189]\n",
      " [ 0.03071683  0.03995365  0.06921923]]\n",
      "[[-1.   -1.   -1.  ]\n",
      " [-1.   -1.   -0.9 ]\n",
      " [-1.   -1.   -0.8 ]\n",
      " ...\n",
      " [ 0.99  0.99  0.97]\n",
      " [ 0.99  0.99  0.98]\n",
      " [ 0.99  0.99  0.99]]\n"
     ]
    }
   ],
   "source": [
    "print(leg_dataset_as_np[0:4])\n",
    "\n",
    "observations = (leg_dataset_as_np[:, 0:3][:])\n",
    "print(observations)\n",
    "actions = (leg_dataset_as_np[:, 3:][:])\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7156/7156 [==============================] - 8s 1ms/step - loss: 0.0248 - accuracy: 0.9077\n",
      "Epoch 2/10\n",
      "7156/7156 [==============================] - 8s 1ms/step - loss: 0.0162 - accuracy: 0.9331\n",
      "Epoch 3/10\n",
      "7156/7156 [==============================] - 8s 1ms/step - loss: 0.0157 - accuracy: 0.9354\n",
      "Epoch 4/10\n",
      "7156/7156 [==============================] - 8s 1ms/step - loss: 0.0153 - accuracy: 0.9367\n",
      "Epoch 5/10\n",
      "7156/7156 [==============================] - 8s 1ms/step - loss: 0.0149 - accuracy: 0.9380\n",
      "Epoch 6/10\n",
      "7156/7156 [==============================] - 8s 1ms/step - loss: 0.0146 - accuracy: 0.9395\n",
      "Epoch 7/10\n",
      "7156/7156 [==============================] - 8s 1ms/step - loss: 0.0142 - accuracy: 0.9412\n",
      "Epoch 8/10\n",
      "7156/7156 [==============================] - 8s 1ms/step - loss: 0.0140 - accuracy: 0.9421\n",
      "Epoch 9/10\n",
      "7156/7156 [==============================] - 8s 1ms/step - loss: 0.0139 - accuracy: 0.9429\n",
      "Epoch 10/10\n",
      "7156/7156 [==============================] - 8s 1ms/step - loss: 0.0138 - accuracy: 0.9436\n",
      "25444/25444 [==============================] - 8s 295us/step - loss: 0.0137 - accuracy: 0.9474\n",
      "Test Loss: [0.013710794039070606, 0.9473649859428406]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_obs, test_obs, train_actions, test_actions = train_test_split(observations, actions, test_size=0.1, shuffle=True)\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3, activation='relu', input_shape=(3,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='tanh')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_obs, train_actions, epochs=10, batch_size=1024)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_loss = model.evaluate(test_obs, test_actions)\n",
    "print('Test Loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: left_legs_model_plus50/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: left_legs_model_plus50/assets\n",
      "2024-06-08 20:49:52.517104: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-06-08 20:49:52.517117: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 8, Total Ops 19, % non-converted = 42.11 %\n",
      " * 8 ARITH ops\n",
      "\n",
      "- arith.constant:    8 occurrences  (f32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 5)\n",
      "  (uq_8: 2)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "model.save(\"left_legs_model_plus50\", overwrite=True)\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"/Users/deniz/PycharmProjects/QRPfRA_Senior_Project/QRPfRA/IK_Models/left_legs_model_plus50\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "left_leg_tflite_model = converter.convert()\n",
    "with open('/Users/deniz/PycharmProjects/QRPfRA_Senior_Project/QRPfRA/IK_Models/left_legs_model_plus50.tflite', 'wb') as f:\n",
    "    f.write(left_leg_tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QRPfRA_Senior_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
